{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network Lab\n",
        "\n",
        "Professor: Rick Chakra\n",
        "\n",
        "TA: Spencer Tilley"
      ],
      "metadata": {
        "id": "4QsKu2zHJx5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to import pytorch and make sure its the right version!"
      ],
      "metadata": {
        "id": "TDQAIuzhV9g-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Pytorch\n",
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "6bluvB4oLX8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15c205ab-fc56-4ae9-9f20-1b2b1c0571d0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If it wont import because torch isn't installed\n",
        "#!pip install torch"
      ],
      "metadata": {
        "id": "rOYTAK8wWFZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets now import the other packages we need"
      ],
      "metadata": {
        "id": "YWNCP4NiWURb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import other packages\n",
        "from numpy import vstack\n",
        "from pandas import read_csv\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import Sigmoid\n",
        "from torch.nn import Module\n",
        "from torch.optim import SGD\n",
        "from torch.nn import BCELoss\n",
        "from torch.nn.init import kaiming_uniform_\n",
        "from torch.nn.init import xavier_uniform_"
      ],
      "metadata": {
        "id": "JMDn20BmLcjR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After importing the remaining packages needed to make a Nural Network we need to set up our dataset to be used by pytorch."
      ],
      "metadata": {
        "id": "EGflJRCNWg36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Difine Dataset\n",
        "class CSVDataset(Dataset):\n",
        "    # load the dataset\n",
        "    def __init__(self, path):\n",
        "        # load the csv file as a dataframe\n",
        "        self.df = read_csv(path, header=0)\n",
        "        # store the inputs and outputs\n",
        "        self.X = self.df.values[:, :-1]\n",
        "        self.y = self.df.values[:, -1]\n",
        "        # ensure input data is floats\n",
        "        self.X = self.X.astype('float32')\n",
        "        # label encode target and ensure the values are floats\n",
        "        self.y = LabelEncoder().fit_transform(self.y)\n",
        "        self.y = self.y.astype('float32')\n",
        "        self.y = self.y.reshape((len(self.y), 1))\n",
        "\n",
        "    # number of rows in the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    # get a row at an index\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.X[idx], self.y[idx]]\n",
        "\n",
        "    # get indexes for train and test rows\n",
        "    def get_splits(self, n_test=0.33):\n",
        "        # determine sizes\n",
        "        test_size = round(n_test * len(self.X))\n",
        "        train_size = len(self.X) - test_size\n",
        "        # calculate the split\n",
        "        return random_split(self, [train_size, test_size])\n",
        "\n",
        "    # get dataset\n",
        "    def get_dataset(self):\n",
        "      return self.df"
      ],
      "metadata": {
        "id": "MvxsUIjeLiDo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we are going to make a function to use the previous function and split our dataset."
      ],
      "metadata": {
        "id": "6in4P_vwWuAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the dataset\n",
        "def prepare_data(path):\n",
        "    # load the dataset\n",
        "    dataset = CSVDataset(path)\n",
        "    # calculate split\n",
        "    train, test = dataset.get_splits()\n",
        "    # prepare data loaders\n",
        "    train_dl = DataLoader(train, batch_size=47, shuffle=True)\n",
        "    test_dl = DataLoader(test, batch_size=116, shuffle=False)\n",
        "\n",
        "    # get df\n",
        "    df = dataset.get_dataset()\n",
        "    return df, train_dl, test_dl"
      ],
      "metadata": {
        "id": "7N23V5qtLy23"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have all of our data functions defined we can call them to prepare our data"
      ],
      "metadata": {
        "id": "wTEycy6tW7bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import and prepare the data\n",
        "# path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
        "path = 'ionosphere.csv'\n",
        "df, train_dl, test_dl = prepare_data(path)\n",
        "print(len(train_dl.dataset), len(test_dl.dataset))"
      ],
      "metadata": {
        "id": "C3x1O1-oLzwe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aed2c433-af9a-47d4-c951-b5488b98eb2d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "235 116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# view dataset\n",
        "print(df)"
      ],
      "metadata": {
        "id": "PL3BQ7vhd6rY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c218c3e4-1796-4ea1-e853-1fab1a8a0df5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     1  2        3        4        5        6        7        8        9  \\\n",
            "0    1  0  0.99539 -0.05889  0.85243  0.02306  0.83398 -0.37708  1.00000   \n",
            "1    1  0  1.00000 -0.18829  0.93035 -0.36156 -0.10868 -0.93597  1.00000   \n",
            "2    1  0  1.00000 -0.03365  1.00000  0.00485  1.00000 -0.12062  0.88965   \n",
            "3    1  0  1.00000 -0.45161  1.00000  1.00000  0.71216 -1.00000  0.00000   \n",
            "4    1  0  1.00000 -0.02401  0.94140  0.06531  0.92106 -0.23255  0.77152   \n",
            "..  .. ..      ...      ...      ...      ...      ...      ...      ...   \n",
            "346  1  0  0.83508  0.08298  0.73739 -0.14706  0.84349 -0.05567  0.90441   \n",
            "347  1  0  0.95113  0.00419  0.95183 -0.02723  0.93438 -0.01920  0.94590   \n",
            "348  1  0  0.94701 -0.00034  0.93207 -0.03227  0.95177 -0.03431  0.95584   \n",
            "349  1  0  0.90608 -0.01657  0.98122 -0.01989  0.95691 -0.03646  0.85746   \n",
            "350  1  0  0.84710  0.13533  0.73638 -0.06151  0.87873  0.08260  0.88928   \n",
            "\n",
            "          10  ...       26       27       28       29       30       31  \\\n",
            "0    0.03760  ... -0.51171  0.41078 -0.46168  0.21266 -0.34090  0.42267   \n",
            "1   -0.04549  ... -0.26569 -0.20468 -0.18401 -0.19040 -0.11593 -0.16626   \n",
            "2    0.01198  ... -0.40220  0.58984 -0.22145  0.43100 -0.17365  0.60436   \n",
            "3    0.00000  ...  0.90695  0.51613  1.00000  1.00000 -0.20099  0.25682   \n",
            "4   -0.16399  ... -0.65158  0.13290 -0.53206  0.02431 -0.62197 -0.05707   \n",
            "..       ...  ...      ...      ...      ...      ...      ...      ...   \n",
            "346 -0.04622  ... -0.04202  0.83479  0.00123  1.00000  0.12815  0.86660   \n",
            "347  0.01606  ...  0.01361  0.93522  0.04925  0.93159  0.08168  0.94066   \n",
            "348  0.02446  ...  0.03193  0.92489  0.02542  0.92120  0.02242  0.92459   \n",
            "349  0.00110  ... -0.02099  0.89147 -0.07760  0.82983 -0.17238  0.96022   \n",
            "350 -0.09139  ... -0.15114  0.81147 -0.04822  0.78207 -0.00703  0.75747   \n",
            "\n",
            "          32       33       34  35  \n",
            "0   -0.54487  0.18641 -0.45300   g  \n",
            "1   -0.06288 -0.13738 -0.02447   b  \n",
            "2   -0.24180  0.56045 -0.38238   g  \n",
            "3    1.00000 -0.32382  1.00000   b  \n",
            "4   -0.59573 -0.04608 -0.65697   g  \n",
            "..       ...      ...      ...  ..  \n",
            "346 -0.10714  0.90546 -0.04307   g  \n",
            "347 -0.00035  0.91483  0.04712   g  \n",
            "348  0.00442  0.92697 -0.00577   g  \n",
            "349 -0.03757  0.87403 -0.16243   g  \n",
            "350 -0.06678  0.85764 -0.06151   g  \n",
            "\n",
            "[351 rows x 35 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our data is ready we need to make our model. Unlike the other model types we have used in this class, we need to actually make our model (This can involve a lot of tuning in real life). For this example lets make 3 hidden layers."
      ],
      "metadata": {
        "id": "z_oIgp0lW5it"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model definition\n",
        "class MLP(Module):\n",
        "    # define model elements\n",
        "    def __init__(self, n_inputs):\n",
        "        super(MLP, self).__init__()\n",
        "        # input to first hidden layer\n",
        "        self.hidden1 = Linear(n_inputs, 10)\n",
        "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
        "        self.act1 = ReLU()\n",
        "        # second hidden layer\n",
        "        self.hidden2 = Linear(10,8)\n",
        "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
        "        self.act2 = ReLU()\n",
        "        # third hidden layer and output\n",
        "        self.hidden3 = Linear(8,1)\n",
        "        xavier_uniform_(self.hidden3.weight)\n",
        "        self.act3 = Sigmoid()\n",
        "\n",
        "    # forward propagate input\n",
        "    def forward(self, X):\n",
        "        # input to first hidden layer\n",
        "        X = self.hidden1(X)\n",
        "        X = self.act1(X)\n",
        "         # second hidden layer\n",
        "        X = self.hidden2(X)\n",
        "        X = self.act2(X)\n",
        "        # third hidden layer and output\n",
        "        X = self.hidden3(X)\n",
        "        X = self.act3(X)\n",
        "        return X"
      ],
      "metadata": {
        "id": "PyR3Gc8RL8YJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to choose how our model will train and improve"
      ],
      "metadata": {
        "id": "IfzY6hUmXp3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "def train_model(train_dl, model):\n",
        "    # define the optimization\n",
        "    criterion = BCELoss()\n",
        "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    for epoch in range(150):\n",
        "      # enumerate mini batches\n",
        "      for i, (inputs, targets) in enumerate(train_dl):\n",
        "          # clear the gradients\n",
        "          optimizer.zero_grad()\n",
        "          # compute the model output\n",
        "          yhat = model(inputs)\n",
        "          # calculate loss\n",
        "          loss = criterion(yhat, targets)\n",
        "          # loss assignment\n",
        "          loss.backward()\n",
        "          # update model weights\n",
        "          optimizer.step()\n",
        "\n",
        "          if i % 3 == 0:\n",
        "              print(f'Training loss. Epoch: {epoch} Batch: {i} Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "sLgUkrtIL9py"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we set up a function to define how to evaluate the model (accuracy)"
      ],
      "metadata": {
        "id": "SllBP3hPX3P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "def evaluate_model(test_dl, model):\n",
        "    predictions, actuals = list(), list()\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\n",
        "        # evaluate the model on the test set\n",
        "        yhat = model(inputs)\n",
        "        # retrieve numpy array\n",
        "        yhat = yhat.detach().numpy()\n",
        "        actual = targets.numpy()\n",
        "        actual = actual.reshape((len(actual), 1))\n",
        "        # round to class values\n",
        "        yhat = yhat.round()\n",
        "        # store\n",
        "        predictions.append(yhat)\n",
        "        actuals.append(actual)\n",
        "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
        "    # calculate accuracy\n",
        "    acc = accuracy_score(actuals, predictions)\n",
        "    return acc"
      ],
      "metadata": {
        "id": "WL8zll3GMBGX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To further show how our model is working lets have the completed model create a prediction on a row of data"
      ],
      "metadata": {
        "id": "YafIowI3YNHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a class prediction for one row of data\n",
        "def predict(row, model):\n",
        "    # convert row to data\n",
        "    row = Tensor([row])\n",
        "    # make prediction\n",
        "    yhat = model(row)\n",
        "    # retrieve numpy array\n",
        "    yhat = yhat.detach().numpy()\n",
        "    return yhat"
      ],
      "metadata": {
        "id": "HkPhXzCrMGD8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now its time to actually run the model and have it use our data!"
      ],
      "metadata": {
        "id": "-Nqz4hbYYb3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the network\n",
        "model = MLP(34)\n",
        "# train the model\n",
        "train_model(train_dl, model)\n",
        "# evaluate the model\n",
        "acc = evaluate_model(test_dl, model)"
      ],
      "metadata": {
        "id": "wGWjuC1OMMYR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3f059cc-c880-4703-d6a7-1937b39615c8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss. Epoch: 0 Batch: 0 Loss: 0.7229156494140625\n",
            "Training loss. Epoch: 0 Batch: 3 Loss: 0.7271541953086853\n",
            "Training loss. Epoch: 1 Batch: 0 Loss: 0.7548994421958923\n",
            "Training loss. Epoch: 1 Batch: 3 Loss: 0.6991768479347229\n",
            "Training loss. Epoch: 2 Batch: 0 Loss: 0.6560651063919067\n",
            "Training loss. Epoch: 2 Batch: 3 Loss: 0.6178153157234192\n",
            "Training loss. Epoch: 3 Batch: 0 Loss: 0.6290187239646912\n",
            "Training loss. Epoch: 3 Batch: 3 Loss: 0.4911806583404541\n",
            "Training loss. Epoch: 4 Batch: 0 Loss: 0.5756160020828247\n",
            "Training loss. Epoch: 4 Batch: 3 Loss: 0.544249951839447\n",
            "Training loss. Epoch: 5 Batch: 0 Loss: 0.5162132382392883\n",
            "Training loss. Epoch: 5 Batch: 3 Loss: 0.5077219009399414\n",
            "Training loss. Epoch: 6 Batch: 0 Loss: 0.4451996386051178\n",
            "Training loss. Epoch: 6 Batch: 3 Loss: 0.5680035948753357\n",
            "Training loss. Epoch: 7 Batch: 0 Loss: 0.5897856950759888\n",
            "Training loss. Epoch: 7 Batch: 3 Loss: 0.5630333423614502\n",
            "Training loss. Epoch: 8 Batch: 0 Loss: 0.4514332115650177\n",
            "Training loss. Epoch: 8 Batch: 3 Loss: 0.448466032743454\n",
            "Training loss. Epoch: 9 Batch: 0 Loss: 0.5020171403884888\n",
            "Training loss. Epoch: 9 Batch: 3 Loss: 0.4730701446533203\n",
            "Training loss. Epoch: 10 Batch: 0 Loss: 0.4402388334274292\n",
            "Training loss. Epoch: 10 Batch: 3 Loss: 0.43996462225914\n",
            "Training loss. Epoch: 11 Batch: 0 Loss: 0.4329046308994293\n",
            "Training loss. Epoch: 11 Batch: 3 Loss: 0.45200103521347046\n",
            "Training loss. Epoch: 12 Batch: 0 Loss: 0.3149152100086212\n",
            "Training loss. Epoch: 12 Batch: 3 Loss: 0.5335792899131775\n",
            "Training loss. Epoch: 13 Batch: 0 Loss: 0.40340569615364075\n",
            "Training loss. Epoch: 13 Batch: 3 Loss: 0.4232962727546692\n",
            "Training loss. Epoch: 14 Batch: 0 Loss: 0.3418624699115753\n",
            "Training loss. Epoch: 14 Batch: 3 Loss: 0.32150793075561523\n",
            "Training loss. Epoch: 15 Batch: 0 Loss: 0.34516021609306335\n",
            "Training loss. Epoch: 15 Batch: 3 Loss: 0.3316642642021179\n",
            "Training loss. Epoch: 16 Batch: 0 Loss: 0.3297984302043915\n",
            "Training loss. Epoch: 16 Batch: 3 Loss: 0.3731284737586975\n",
            "Training loss. Epoch: 17 Batch: 0 Loss: 0.4063727557659149\n",
            "Training loss. Epoch: 17 Batch: 3 Loss: 0.3484072685241699\n",
            "Training loss. Epoch: 18 Batch: 0 Loss: 0.39495381712913513\n",
            "Training loss. Epoch: 18 Batch: 3 Loss: 0.4035722017288208\n",
            "Training loss. Epoch: 19 Batch: 0 Loss: 0.325296550989151\n",
            "Training loss. Epoch: 19 Batch: 3 Loss: 0.344248503446579\n",
            "Training loss. Epoch: 20 Batch: 0 Loss: 0.32343730330467224\n",
            "Training loss. Epoch: 20 Batch: 3 Loss: 0.3539259135723114\n",
            "Training loss. Epoch: 21 Batch: 0 Loss: 0.29980841279029846\n",
            "Training loss. Epoch: 21 Batch: 3 Loss: 0.28703591227531433\n",
            "Training loss. Epoch: 22 Batch: 0 Loss: 0.3210510313510895\n",
            "Training loss. Epoch: 22 Batch: 3 Loss: 0.3394528925418854\n",
            "Training loss. Epoch: 23 Batch: 0 Loss: 0.29866230487823486\n",
            "Training loss. Epoch: 23 Batch: 3 Loss: 0.3535645604133606\n",
            "Training loss. Epoch: 24 Batch: 0 Loss: 0.2345225214958191\n",
            "Training loss. Epoch: 24 Batch: 3 Loss: 0.27307870984077454\n",
            "Training loss. Epoch: 25 Batch: 0 Loss: 0.2988635301589966\n",
            "Training loss. Epoch: 25 Batch: 3 Loss: 0.30627402663230896\n",
            "Training loss. Epoch: 26 Batch: 0 Loss: 0.20304983854293823\n",
            "Training loss. Epoch: 26 Batch: 3 Loss: 0.2696106731891632\n",
            "Training loss. Epoch: 27 Batch: 0 Loss: 0.2856070101261139\n",
            "Training loss. Epoch: 27 Batch: 3 Loss: 0.24118240177631378\n",
            "Training loss. Epoch: 28 Batch: 0 Loss: 0.3087054491043091\n",
            "Training loss. Epoch: 28 Batch: 3 Loss: 0.2829190790653229\n",
            "Training loss. Epoch: 29 Batch: 0 Loss: 0.21122246980667114\n",
            "Training loss. Epoch: 29 Batch: 3 Loss: 0.24031580984592438\n",
            "Training loss. Epoch: 30 Batch: 0 Loss: 0.33934611082077026\n",
            "Training loss. Epoch: 30 Batch: 3 Loss: 0.24178862571716309\n",
            "Training loss. Epoch: 31 Batch: 0 Loss: 0.2737498879432678\n",
            "Training loss. Epoch: 31 Batch: 3 Loss: 0.2662803530693054\n",
            "Training loss. Epoch: 32 Batch: 0 Loss: 0.2221217006444931\n",
            "Training loss. Epoch: 32 Batch: 3 Loss: 0.22052131593227386\n",
            "Training loss. Epoch: 33 Batch: 0 Loss: 0.21233615279197693\n",
            "Training loss. Epoch: 33 Batch: 3 Loss: 0.16644947230815887\n",
            "Training loss. Epoch: 34 Batch: 0 Loss: 0.18202942609786987\n",
            "Training loss. Epoch: 34 Batch: 3 Loss: 0.20831574499607086\n",
            "Training loss. Epoch: 35 Batch: 0 Loss: 0.14870049059391022\n",
            "Training loss. Epoch: 35 Batch: 3 Loss: 0.25969356298446655\n",
            "Training loss. Epoch: 36 Batch: 0 Loss: 0.19964486360549927\n",
            "Training loss. Epoch: 36 Batch: 3 Loss: 0.24341875314712524\n",
            "Training loss. Epoch: 37 Batch: 0 Loss: 0.1265372782945633\n",
            "Training loss. Epoch: 37 Batch: 3 Loss: 0.23904727399349213\n",
            "Training loss. Epoch: 38 Batch: 0 Loss: 0.22669067978858948\n",
            "Training loss. Epoch: 38 Batch: 3 Loss: 0.22866711020469666\n",
            "Training loss. Epoch: 39 Batch: 0 Loss: 0.230881005525589\n",
            "Training loss. Epoch: 39 Batch: 3 Loss: 0.17424936592578888\n",
            "Training loss. Epoch: 40 Batch: 0 Loss: 0.16801513731479645\n",
            "Training loss. Epoch: 40 Batch: 3 Loss: 0.20371533930301666\n",
            "Training loss. Epoch: 41 Batch: 0 Loss: 0.09145139157772064\n",
            "Training loss. Epoch: 41 Batch: 3 Loss: 0.2767083942890167\n",
            "Training loss. Epoch: 42 Batch: 0 Loss: 0.13939857482910156\n",
            "Training loss. Epoch: 42 Batch: 3 Loss: 0.19287963211536407\n",
            "Training loss. Epoch: 43 Batch: 0 Loss: 0.18735362589359283\n",
            "Training loss. Epoch: 43 Batch: 3 Loss: 0.19977782666683197\n",
            "Training loss. Epoch: 44 Batch: 0 Loss: 0.1652401089668274\n",
            "Training loss. Epoch: 44 Batch: 3 Loss: 0.20249778032302856\n",
            "Training loss. Epoch: 45 Batch: 0 Loss: 0.12088470160961151\n",
            "Training loss. Epoch: 45 Batch: 3 Loss: 0.10619772225618362\n",
            "Training loss. Epoch: 46 Batch: 0 Loss: 0.16803906857967377\n",
            "Training loss. Epoch: 46 Batch: 3 Loss: 0.2504602074623108\n",
            "Training loss. Epoch: 47 Batch: 0 Loss: 0.19742454588413239\n",
            "Training loss. Epoch: 47 Batch: 3 Loss: 0.07684991508722305\n",
            "Training loss. Epoch: 48 Batch: 0 Loss: 0.10590482503175735\n",
            "Training loss. Epoch: 48 Batch: 3 Loss: 0.24256476759910583\n",
            "Training loss. Epoch: 49 Batch: 0 Loss: 0.1549624353647232\n",
            "Training loss. Epoch: 49 Batch: 3 Loss: 0.20339590311050415\n",
            "Training loss. Epoch: 50 Batch: 0 Loss: 0.20441848039627075\n",
            "Training loss. Epoch: 50 Batch: 3 Loss: 0.06142328307032585\n",
            "Training loss. Epoch: 51 Batch: 0 Loss: 0.0834847167134285\n",
            "Training loss. Epoch: 51 Batch: 3 Loss: 0.1861845850944519\n",
            "Training loss. Epoch: 52 Batch: 0 Loss: 0.21049310266971588\n",
            "Training loss. Epoch: 52 Batch: 3 Loss: 0.10020612180233002\n",
            "Training loss. Epoch: 53 Batch: 0 Loss: 0.23111072182655334\n",
            "Training loss. Epoch: 53 Batch: 3 Loss: 0.10733918845653534\n",
            "Training loss. Epoch: 54 Batch: 0 Loss: 0.1281132847070694\n",
            "Training loss. Epoch: 54 Batch: 3 Loss: 0.13260312378406525\n",
            "Training loss. Epoch: 55 Batch: 0 Loss: 0.15468186140060425\n",
            "Training loss. Epoch: 55 Batch: 3 Loss: 0.1262422353029251\n",
            "Training loss. Epoch: 56 Batch: 0 Loss: 0.17237472534179688\n",
            "Training loss. Epoch: 56 Batch: 3 Loss: 0.08161728829145432\n",
            "Training loss. Epoch: 57 Batch: 0 Loss: 0.12888255715370178\n",
            "Training loss. Epoch: 57 Batch: 3 Loss: 0.111726313829422\n",
            "Training loss. Epoch: 58 Batch: 0 Loss: 0.16608327627182007\n",
            "Training loss. Epoch: 58 Batch: 3 Loss: 0.1269250363111496\n",
            "Training loss. Epoch: 59 Batch: 0 Loss: 0.2500375509262085\n",
            "Training loss. Epoch: 59 Batch: 3 Loss: 0.1424277275800705\n",
            "Training loss. Epoch: 60 Batch: 0 Loss: 0.14521880447864532\n",
            "Training loss. Epoch: 60 Batch: 3 Loss: 0.10352014005184174\n",
            "Training loss. Epoch: 61 Batch: 0 Loss: 0.14776292443275452\n",
            "Training loss. Epoch: 61 Batch: 3 Loss: 0.17181003093719482\n",
            "Training loss. Epoch: 62 Batch: 0 Loss: 0.17394638061523438\n",
            "Training loss. Epoch: 62 Batch: 3 Loss: 0.14827831089496613\n",
            "Training loss. Epoch: 63 Batch: 0 Loss: 0.08615954220294952\n",
            "Training loss. Epoch: 63 Batch: 3 Loss: 0.09707570821046829\n",
            "Training loss. Epoch: 64 Batch: 0 Loss: 0.139789879322052\n",
            "Training loss. Epoch: 64 Batch: 3 Loss: 0.10490421205759048\n",
            "Training loss. Epoch: 65 Batch: 0 Loss: 0.12125807255506516\n",
            "Training loss. Epoch: 65 Batch: 3 Loss: 0.11436255276203156\n",
            "Training loss. Epoch: 66 Batch: 0 Loss: 0.08009430021047592\n",
            "Training loss. Epoch: 66 Batch: 3 Loss: 0.17831012606620789\n",
            "Training loss. Epoch: 67 Batch: 0 Loss: 0.13724258542060852\n",
            "Training loss. Epoch: 67 Batch: 3 Loss: 0.12070931494235992\n",
            "Training loss. Epoch: 68 Batch: 0 Loss: 0.0637156143784523\n",
            "Training loss. Epoch: 68 Batch: 3 Loss: 0.11130311340093613\n",
            "Training loss. Epoch: 69 Batch: 0 Loss: 0.08867131173610687\n",
            "Training loss. Epoch: 69 Batch: 3 Loss: 0.09626815468072891\n",
            "Training loss. Epoch: 70 Batch: 0 Loss: 0.13580358028411865\n",
            "Training loss. Epoch: 70 Batch: 3 Loss: 0.10488373041152954\n",
            "Training loss. Epoch: 71 Batch: 0 Loss: 0.06344978511333466\n",
            "Training loss. Epoch: 71 Batch: 3 Loss: 0.1259126514196396\n",
            "Training loss. Epoch: 72 Batch: 0 Loss: 0.09585469961166382\n",
            "Training loss. Epoch: 72 Batch: 3 Loss: 0.142928808927536\n",
            "Training loss. Epoch: 73 Batch: 0 Loss: 0.17399904131889343\n",
            "Training loss. Epoch: 73 Batch: 3 Loss: 0.07065307348966599\n",
            "Training loss. Epoch: 74 Batch: 0 Loss: 0.08997762948274612\n",
            "Training loss. Epoch: 74 Batch: 3 Loss: 0.08320819586515427\n",
            "Training loss. Epoch: 75 Batch: 0 Loss: 0.10761908441781998\n",
            "Training loss. Epoch: 75 Batch: 3 Loss: 0.12247098982334137\n",
            "Training loss. Epoch: 76 Batch: 0 Loss: 0.09753531962633133\n",
            "Training loss. Epoch: 76 Batch: 3 Loss: 0.04772774875164032\n",
            "Training loss. Epoch: 77 Batch: 0 Loss: 0.10348763316869736\n",
            "Training loss. Epoch: 77 Batch: 3 Loss: 0.12333231419324875\n",
            "Training loss. Epoch: 78 Batch: 0 Loss: 0.0868450254201889\n",
            "Training loss. Epoch: 78 Batch: 3 Loss: 0.09632088243961334\n",
            "Training loss. Epoch: 79 Batch: 0 Loss: 0.14538127183914185\n",
            "Training loss. Epoch: 79 Batch: 3 Loss: 0.07312143594026566\n",
            "Training loss. Epoch: 80 Batch: 0 Loss: 0.08207867294549942\n",
            "Training loss. Epoch: 80 Batch: 3 Loss: 0.13817019760608673\n",
            "Training loss. Epoch: 81 Batch: 0 Loss: 0.10963259637355804\n",
            "Training loss. Epoch: 81 Batch: 3 Loss: 0.06628204882144928\n",
            "Training loss. Epoch: 82 Batch: 0 Loss: 0.08913104981184006\n",
            "Training loss. Epoch: 82 Batch: 3 Loss: 0.09526730328798294\n",
            "Training loss. Epoch: 83 Batch: 0 Loss: 0.08218849450349808\n",
            "Training loss. Epoch: 83 Batch: 3 Loss: 0.09147290140390396\n",
            "Training loss. Epoch: 84 Batch: 0 Loss: 0.06355147808790207\n",
            "Training loss. Epoch: 84 Batch: 3 Loss: 0.14422713220119476\n",
            "Training loss. Epoch: 85 Batch: 0 Loss: 0.10459479689598083\n",
            "Training loss. Epoch: 85 Batch: 3 Loss: 0.10924401879310608\n",
            "Training loss. Epoch: 86 Batch: 0 Loss: 0.03325412794947624\n",
            "Training loss. Epoch: 86 Batch: 3 Loss: 0.0955638661980629\n",
            "Training loss. Epoch: 87 Batch: 0 Loss: 0.04884150251746178\n",
            "Training loss. Epoch: 87 Batch: 3 Loss: 0.15307433903217316\n",
            "Training loss. Epoch: 88 Batch: 0 Loss: 0.12472688406705856\n",
            "Training loss. Epoch: 88 Batch: 3 Loss: 0.07107630372047424\n",
            "Training loss. Epoch: 89 Batch: 0 Loss: 0.0522335022687912\n",
            "Training loss. Epoch: 89 Batch: 3 Loss: 0.035494115203619\n",
            "Training loss. Epoch: 90 Batch: 0 Loss: 0.03953048959374428\n",
            "Training loss. Epoch: 90 Batch: 3 Loss: 0.09287247806787491\n",
            "Training loss. Epoch: 91 Batch: 0 Loss: 0.05287979915738106\n",
            "Training loss. Epoch: 91 Batch: 3 Loss: 0.11253379285335541\n",
            "Training loss. Epoch: 92 Batch: 0 Loss: 0.18425342440605164\n",
            "Training loss. Epoch: 92 Batch: 3 Loss: 0.05489484220743179\n",
            "Training loss. Epoch: 93 Batch: 0 Loss: 0.0735827386379242\n",
            "Training loss. Epoch: 93 Batch: 3 Loss: 0.056764740496873856\n",
            "Training loss. Epoch: 94 Batch: 0 Loss: 0.05392555892467499\n",
            "Training loss. Epoch: 94 Batch: 3 Loss: 0.14132404327392578\n",
            "Training loss. Epoch: 95 Batch: 0 Loss: 0.09352955967187881\n",
            "Training loss. Epoch: 95 Batch: 3 Loss: 0.05618519335985184\n",
            "Training loss. Epoch: 96 Batch: 0 Loss: 0.12303537875413895\n",
            "Training loss. Epoch: 96 Batch: 3 Loss: 0.09698814153671265\n",
            "Training loss. Epoch: 97 Batch: 0 Loss: 0.059709589928388596\n",
            "Training loss. Epoch: 97 Batch: 3 Loss: 0.05827857181429863\n",
            "Training loss. Epoch: 98 Batch: 0 Loss: 0.0685681402683258\n",
            "Training loss. Epoch: 98 Batch: 3 Loss: 0.11189448833465576\n",
            "Training loss. Epoch: 99 Batch: 0 Loss: 0.13337235152721405\n",
            "Training loss. Epoch: 99 Batch: 3 Loss: 0.03874998539686203\n",
            "Training loss. Epoch: 100 Batch: 0 Loss: 0.045416392385959625\n",
            "Training loss. Epoch: 100 Batch: 3 Loss: 0.024656258523464203\n",
            "Training loss. Epoch: 101 Batch: 0 Loss: 0.042850520461797714\n",
            "Training loss. Epoch: 101 Batch: 3 Loss: 0.11264529824256897\n",
            "Training loss. Epoch: 102 Batch: 0 Loss: 0.03703315556049347\n",
            "Training loss. Epoch: 102 Batch: 3 Loss: 0.05626988038420677\n",
            "Training loss. Epoch: 103 Batch: 0 Loss: 0.08165837824344635\n",
            "Training loss. Epoch: 103 Batch: 3 Loss: 0.05603247135877609\n",
            "Training loss. Epoch: 104 Batch: 0 Loss: 0.024297090247273445\n",
            "Training loss. Epoch: 104 Batch: 3 Loss: 0.1276889592409134\n",
            "Training loss. Epoch: 105 Batch: 0 Loss: 0.11036039888858795\n",
            "Training loss. Epoch: 105 Batch: 3 Loss: 0.08563203364610672\n",
            "Training loss. Epoch: 106 Batch: 0 Loss: 0.09707610309123993\n",
            "Training loss. Epoch: 106 Batch: 3 Loss: 0.035566546022892\n",
            "Training loss. Epoch: 107 Batch: 0 Loss: 0.07051017880439758\n",
            "Training loss. Epoch: 107 Batch: 3 Loss: 0.036728352308273315\n",
            "Training loss. Epoch: 108 Batch: 0 Loss: 0.022007916122674942\n",
            "Training loss. Epoch: 108 Batch: 3 Loss: 0.18233130872249603\n",
            "Training loss. Epoch: 109 Batch: 0 Loss: 0.03342358022928238\n",
            "Training loss. Epoch: 109 Batch: 3 Loss: 0.04807077720761299\n",
            "Training loss. Epoch: 110 Batch: 0 Loss: 0.06910806149244308\n",
            "Training loss. Epoch: 110 Batch: 3 Loss: 0.03745514154434204\n",
            "Training loss. Epoch: 111 Batch: 0 Loss: 0.11955870687961578\n",
            "Training loss. Epoch: 111 Batch: 3 Loss: 0.1088952124118805\n",
            "Training loss. Epoch: 112 Batch: 0 Loss: 0.10412637144327164\n",
            "Training loss. Epoch: 112 Batch: 3 Loss: 0.03429393470287323\n",
            "Training loss. Epoch: 113 Batch: 0 Loss: 0.04638310521841049\n",
            "Training loss. Epoch: 113 Batch: 3 Loss: 0.03369026631116867\n",
            "Training loss. Epoch: 114 Batch: 0 Loss: 0.04283826798200607\n",
            "Training loss. Epoch: 114 Batch: 3 Loss: 0.03415858373045921\n",
            "Training loss. Epoch: 115 Batch: 0 Loss: 0.05148039385676384\n",
            "Training loss. Epoch: 115 Batch: 3 Loss: 0.023373959586024284\n",
            "Training loss. Epoch: 116 Batch: 0 Loss: 0.04107258841395378\n",
            "Training loss. Epoch: 116 Batch: 3 Loss: 0.15193982422351837\n",
            "Training loss. Epoch: 117 Batch: 0 Loss: 0.047781527042388916\n",
            "Training loss. Epoch: 117 Batch: 3 Loss: 0.07822158187627792\n",
            "Training loss. Epoch: 118 Batch: 0 Loss: 0.03091573156416416\n",
            "Training loss. Epoch: 118 Batch: 3 Loss: 0.049961261451244354\n",
            "Training loss. Epoch: 119 Batch: 0 Loss: 0.11281704157590866\n",
            "Training loss. Epoch: 119 Batch: 3 Loss: 0.07024135440587997\n",
            "Training loss. Epoch: 120 Batch: 0 Loss: 0.04483283311128616\n",
            "Training loss. Epoch: 120 Batch: 3 Loss: 0.055116333067417145\n",
            "Training loss. Epoch: 121 Batch: 0 Loss: 0.14122608304023743\n",
            "Training loss. Epoch: 121 Batch: 3 Loss: 0.029735039919614792\n",
            "Training loss. Epoch: 122 Batch: 0 Loss: 0.0808858647942543\n",
            "Training loss. Epoch: 122 Batch: 3 Loss: 0.11412978917360306\n",
            "Training loss. Epoch: 123 Batch: 0 Loss: 0.034124955534935\n",
            "Training loss. Epoch: 123 Batch: 3 Loss: 0.06960272789001465\n",
            "Training loss. Epoch: 124 Batch: 0 Loss: 0.029347945004701614\n",
            "Training loss. Epoch: 124 Batch: 3 Loss: 0.0956326350569725\n",
            "Training loss. Epoch: 125 Batch: 0 Loss: 0.02139856293797493\n",
            "Training loss. Epoch: 125 Batch: 3 Loss: 0.03319146856665611\n",
            "Training loss. Epoch: 126 Batch: 0 Loss: 0.0292049590498209\n",
            "Training loss. Epoch: 126 Batch: 3 Loss: 0.11336808651685715\n",
            "Training loss. Epoch: 127 Batch: 0 Loss: 0.08783657848834991\n",
            "Training loss. Epoch: 127 Batch: 3 Loss: 0.10729272663593292\n",
            "Training loss. Epoch: 128 Batch: 0 Loss: 0.10289309918880463\n",
            "Training loss. Epoch: 128 Batch: 3 Loss: 0.07995869219303131\n",
            "Training loss. Epoch: 129 Batch: 0 Loss: 0.08865788578987122\n",
            "Training loss. Epoch: 129 Batch: 3 Loss: 0.035916876047849655\n",
            "Training loss. Epoch: 130 Batch: 0 Loss: 0.03240511938929558\n",
            "Training loss. Epoch: 130 Batch: 3 Loss: 0.028916651383042336\n",
            "Training loss. Epoch: 131 Batch: 0 Loss: 0.020276708528399467\n",
            "Training loss. Epoch: 131 Batch: 3 Loss: 0.07862796634435654\n",
            "Training loss. Epoch: 132 Batch: 0 Loss: 0.04132560268044472\n",
            "Training loss. Epoch: 132 Batch: 3 Loss: 0.04466172307729721\n",
            "Training loss. Epoch: 133 Batch: 0 Loss: 0.027835018932819366\n",
            "Training loss. Epoch: 133 Batch: 3 Loss: 0.12060344964265823\n",
            "Training loss. Epoch: 134 Batch: 0 Loss: 0.03777199983596802\n",
            "Training loss. Epoch: 134 Batch: 3 Loss: 0.06122754141688347\n",
            "Training loss. Epoch: 135 Batch: 0 Loss: 0.023080972954630852\n",
            "Training loss. Epoch: 135 Batch: 3 Loss: 0.032303258776664734\n",
            "Training loss. Epoch: 136 Batch: 0 Loss: 0.04202162101864815\n",
            "Training loss. Epoch: 136 Batch: 3 Loss: 0.02307269163429737\n",
            "Training loss. Epoch: 137 Batch: 0 Loss: 0.07694882899522781\n",
            "Training loss. Epoch: 137 Batch: 3 Loss: 0.04439540579915047\n",
            "Training loss. Epoch: 138 Batch: 0 Loss: 0.02653426118195057\n",
            "Training loss. Epoch: 138 Batch: 3 Loss: 0.13066035509109497\n",
            "Training loss. Epoch: 139 Batch: 0 Loss: 0.041783835738897324\n",
            "Training loss. Epoch: 139 Batch: 3 Loss: 0.039140861481428146\n",
            "Training loss. Epoch: 140 Batch: 0 Loss: 0.02420937456190586\n",
            "Training loss. Epoch: 140 Batch: 3 Loss: 0.03267153352499008\n",
            "Training loss. Epoch: 141 Batch: 0 Loss: 0.12301568686962128\n",
            "Training loss. Epoch: 141 Batch: 3 Loss: 0.041099030524492264\n",
            "Training loss. Epoch: 142 Batch: 0 Loss: 0.10562720894813538\n",
            "Training loss. Epoch: 142 Batch: 3 Loss: 0.07973917573690414\n",
            "Training loss. Epoch: 143 Batch: 0 Loss: 0.06176331639289856\n",
            "Training loss. Epoch: 143 Batch: 3 Loss: 0.03978532925248146\n",
            "Training loss. Epoch: 144 Batch: 0 Loss: 0.03704677149653435\n",
            "Training loss. Epoch: 144 Batch: 3 Loss: 0.10307218879461288\n",
            "Training loss. Epoch: 145 Batch: 0 Loss: 0.03582725301384926\n",
            "Training loss. Epoch: 145 Batch: 3 Loss: 0.020006675273180008\n",
            "Training loss. Epoch: 146 Batch: 0 Loss: 0.03357256203889847\n",
            "Training loss. Epoch: 146 Batch: 3 Loss: 0.014078238047659397\n",
            "Training loss. Epoch: 147 Batch: 0 Loss: 0.01406980212777853\n",
            "Training loss. Epoch: 147 Batch: 3 Loss: 0.03128189221024513\n",
            "Training loss. Epoch: 148 Batch: 0 Loss: 0.02626940608024597\n",
            "Training loss. Epoch: 148 Batch: 3 Loss: 0.14672499895095825\n",
            "Training loss. Epoch: 149 Batch: 0 Loss: 0.1118023619055748\n",
            "Training loss. Epoch: 149 Batch: 3 Loss: 0.03422771766781807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print final model accuracy\n",
        "print('Final Model Accuracy: %.3f' % acc)"
      ],
      "metadata": {
        "id": "Ukji0aqLhWyw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03d95afe-7dde-4266-fba0-5319b03c6635"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Model Accuracy: 0.905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a single prediction (expect class=1)\n",
        "row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
        "yhat = predict(row, model)\n",
        "print('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))"
      ],
      "metadata": {
        "id": "6j2MfmIlMP4D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "922b2115-2364-4b91-c443-8ae762b3cc7e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: 0.995 (class=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the reasons that we use functions so much in this section is it allows for an easier time making different models and comparing how they preform!"
      ],
      "metadata": {
        "id": "8fLAQFd3YByp"
      }
    }
  ]
}