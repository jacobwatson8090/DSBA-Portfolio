---
title: "Project No. 2"
author: "Evelyn Guidry, Jacob Watson, Joseph Obonyo"
date: '2023-12-02'
output: pdf_document
---

# Face Recognition

## Data

```{r}
load("data/faces.Rdata")
```

## A:

### First Plotting Regular Images

```{r}
par(mfrow=c(2,2), mar=c(0.1,0.1,0.1,0.1))
face.list = list(15,40,167,198)
for (i in face.list){
  img.mat = matrix(data=faces[,i],nrow=100,ncol=100)
  image(z=t(apply(img.mat,2,rev)), col=gray.colors(100))
}
```

### Plotting Mean Faces

```{r}
par(mfrow=c(1,2), mar=c(1.5,1.5,1.5,1.5))
# Male Mean Face
mean.face1=rowMeans(faces[,1:100])
mean.face1=matrix(data=mean.face1,nrow=100,ncol=100)
image(z=t(apply(mean.face1,2,rev)), col=gray.colors(100), main="Male Mean Face")
# Female Mean Face
mean.face2=rowMeans(faces[,101:200])
mean.face2=matrix(data=mean.face2,nrow=100,ncol=100)
image(z=t(apply(mean.face2,2,rev)), col=gray.colors(100), main="Female Mean Face")
```

The male mean face seems to be much closer to the chin while the female face\
includes the shoulders. The female mean face has much longer hair than the male face.

## B

```{r}
par(mar=c(1,1,1,1))
mean.df = rowMeans(faces[,1:200])
mean.mat = matrix(data=mean.df,nrow=100,ncol=100)
image(z=t(apply(mean.mat,2,rev)), col=gray.colors(100), main="Mean Face (Whole Dataset)")
```


```{r}
D = data.matrix(faces)

pr.faces = princomp(D,cor=F,scores=TRUE)
eigenvectors = pr.faces$loadings
eigenvalues = pr.faces$sdev^2
plot(1:200,eigenvalues, main="Eigenvalues")
```

```{r}
# Finding number of eigenvalues needed to express 80% of variation in the dataset
sum(eigvals[1:24])/200
```
24 eigenvalues are needed to express 80% of the variation in the data set.
```{r}
# Plot eigenfaces
D_new = pr.faces$scores
par(mfrow=c(1,3), mar=c(0.5,0.5,0.5,0.5))
for (i in 1:3){
  eigenface = matrix(D_new[,i],nrow=100,ncol=100)
  image(z=t(apply(eigenface,2,rev)), col=gray.colors(100))
}
```

The first two components seem to differentiate between male and female. While the first image component has short hair and a\ more pronounced jaw, the second component has a smaller face and at least shoulder length hair. This intuitively aligns with the\ fact that perhaps the most important distinguisher among the 200 images would be the gender of the person photographed. The\ third component seems to be almost a negative of the first two components, and has very noticeable shoulders.


## C

```{r}
# Plot Principal Components 1 and 2, and 1 and 3, Color by Gender
gender = rep(c(1,2), each=100)
plot.df = data.frame(pc1=eigenvectors[,1], pc2=eigenvectors[,2], pc3=eigenvectors[,3], gender=gender)
par(mfrow=c(1,2))
plot(plot.df$pc1, plot.df$pc2, col=plot.df$gender, pch=16, main="PC1 and PC2", xlab = "PC1", ylab="PC2")
legend("topleft", legend = c("Male", "Female"), col = 1:2, pch = 16, title = "Gender")
plot(plot.df$pc1, plot.df$pc3, col=plot.df$gender, pch=16, main="PC1 and PC3", xlab = "PC1", ylab="PC3")
```
```{r}
# Plot Principal Components 1 and 2, and 1 and 3, Color by Shoulder
par(mfrow=c(1,2))
plot(plot.df$pc1, plot.df$pc2, col=as.factor(shoulder), pch=16, main="PC1 and PC2", xlab = "PC1", ylab="PC2")
legend("topleft", legend = c("No shoulders", "Shoulders"), col = 1:2, pch = 16, title = "Shoulders")
plot(plot.df$pc1, plot.df$pc3, col=as.factor(shoulder), pch=16, main="PC1 and PC3", xlab = "PC1", ylab="PC3")
```
When comparing the PC1 vs. PC2 graphs, the gender and shoulder groupings seem to create very similar graphs. The PC1 vs. PC3\ graphs also appear to be a bit similar when comparing the groupings of genders and shoulders, however the shoulder="TRUE"\ values seem to be more centered towards the middle range of PC3, while gender is a bit more evenly split.

## D
### PCR
```{r}
library(pls)
set.seed(1)
```
```{r}
# Transposing data and creating indicator variable
faces.t = t(faces)
index = rep(c(-1,1), each=100)
train.df = data.frame(faces.t, index)

# Fitting model
pcr.fit = pcr(index~., data=train.df, validation="LOO")
validationplot(pcr.fit, val.type="MSEP")
```
```{r}
print("Lowest MSEP and Number of Components:")
MSEP(pcr.fit)$val[1,1,][which.min(MSEP(pcr.fit)$val[1,1,])]
```
```{r}
# Calculating Training Error
#pred.pcr = predict(pcr.fit, newdata=train.df)
#predicted.labels = ifelse(pred.pcr > 0, 1, -1)
#train.error = mean(predicted.labels != train.df$index)
#print(paste("Training Error Rate:", train.error))
```

## E
### PLS
```{r}
# Fitting model
pls.fit = plsr(index~., data=train.df, validation="LOO")
validationplot(pls.fit, val.type="MSEP")
```
```{r}
print("Lowest MSEP and Number of Components:")
MSEP(pls.fit)$val[1,1,][which.min(MSEP(pls.fit)$val[1,1,])]
```
PCR creates components using linear combinations of variables without considering  
the response variable. PLS does take the response variable into account and therefore  
often leads to models that are able to fit the response variable with fewer components.


## F
### QDA
```{r}
library(MASS)
```
```{r}
# Get first 5 principal component to use for QDA
qda.mat = eigenvectors[,1:5]
pc.df = data.frame(qda.mat, index)
qda.fit = qda(index~., data=pc.df)
qda.fit
```
```{r}
qda.pred = predict(qda.fit, newdata = pc.df)$class
train.error = mean(qda.pred != pc.df$index)
print(paste("Training Error Rate: ", round(train.error, 4)))
```












